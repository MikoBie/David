---
title: "R Notebook"
output: html_notebook
---
```{r load_packages}
library(rtweet)
library(dplyr)
library(magrittr)
library(tidyverse)
library(tidytext)
library(feather)
source("Twitter_token.R")
```

The idea is to take a sample of people and measure their complexity with different measures. Compare if they will socre on all scales or there will be people who score only on one or two.

## Getting tweets

The idea is to validate hypothesis that people who follow only either CNN or FOX are less complex the ones who follow both sources. First thing first. Download some tweets.

```{r sources}
## Sample of 15000 CNN followers, it's maximum, however there is retryonratelimit argument
CNN_followers <- get_followers("CNN",
                               n=15000,
                               parse=TRUE,
                               retryonratelimit = TRUE)

print("Sleeping after CNN followers. Do not wake up!")
Sys.sleep(900)

## Sample of 15000 Fox News followers, it's maximum, however there is retryonratelimit argument
FOX_followers <- get_followers("FoxNews",
                               n=15000,
                               parse=TRUE,
                               retryonratelimit = TRUE)

print("Sleeping after FOX followers. Do not wake up!")
Sys.sleep(900)

## Lookup CNN followers profiles. Maximum is 90000 per 15 minutes
CNN_followers <- CNN_followers %$%
  user_id %>%
  lookup_users()
write_feather(CNN_followers,"CNN_followers")
print("Sleeping after CNN followers. Do not wake up!")
Sys.sleep(900)

## Lookup Fox News followers profiles. Maximum is 90000 per 15 minutes
FOX_followers <- FOX_followers %$%
  user_id %>%
  lookup_users
write_feather(FOX_followers,"FOX_followers")
print("Sleeping after FOX followers. Do not wake up!")
Sys.sleep(900)

## Check if CNN followers follow Fox News. The lookup friendship limit is exactly the same as app limit, so therefore I am just looking one by one. It is acctualy 180 but better safe than sorry.
CNN_following_FOX_all <- NULL
j <- 0
for (i in CNN_followers %$% screen_name){
  CNN_following_FOX <- lookup_friendships(i, "FoxNews") %>%
  filter(variable=="screen_name" | variable=="following")
  CNN_following_FOX_all <- bind_rows(CNN_following_FOX_all,CNN_following_FOX)
  print(j <- j+1)
  if (j/179 == j%/%179) {
    print("Sleeping during CNN following FOX. Do not wake up!")
    Sys.sleep(900)
  }
}
write_feather(CNN_following_FOX_all,"CNN_following_FOX_all")
print("Sleeping after CNN following FOX. Do not wake up!")
Sys.sleep(900)

## Check if FOX followers follow CNN. The lookup friendship limit is exactly the same as app limit, so therefore I am just looking one by one. It is acctualy 180 but better safe than sorry.
FOX_following_CNN_all <- NULL
j <- 0
for (i in FOX_followers %$% screen_name){
  FOX_following_CNN <- lookup_friendships(i, "CNN") %>%
  filter(variable=="screen_name" | variable=="following")
  FOX_following_CNN_all <- bind_rows(FOX_following_CNN_all,FOX_following_CNN)
  print(j <- j+1)
  if (j/179 == j%/%179) {
    print("Sleeping during FOX following CNN. Do not wake up!")
    Sys.sleep(900)
  }
}
write_feather(FOX_following_CNN_all,"FOX_following_CNN_all")
print("Sleeping after FOX following CNN. Do not wake up!")
Sys.sleep(900)

## Get timelines of Fox News followers. The limit is acctually 180 but better safe than sorry. It is so because it is app limit not timelines. In other words app can connect to Twitter only 180 times within 15 minutes

FOX_followers <- read_feather("FOX_followers") %>% slice(1:7500)
FOX_timeline_all <- data_frame(screen_name="")
j <- 0
for (i in FOX_followers %>% filter(protected!=TRUE) %$% screen_name) {
  FOX_timeline <- get_timeline(i, n = 100)
  FOX_timeline_all <- bind_rows(FOX_timeline_all,FOX_timeline)
  ## if rate limit exhausted, then wait to rate limit reset
  if (j / 179 == j %/% 179) {
    print("Sleeping, during FOX timelines. Do not disturb!")
    Sys.sleep(900)
  }
  print(j <- j+1)
}
write_as_csv(FOX_timeline_all,"FOX_timelines_all.csv_1")
write_feather(FOX_timeline_all %>% select(-hashtags) ,"FOX_timelines_all_1")

FOX_timeline_all <- read_twitter_csv("FOX_timelines_all.csv_1") %>%
  bind_rows(read_twitter_csv("FOX_timelines_all.csv_2"))

FOX_timeline_all %>%
  write_as_csv("FOX_timeline_all.csv")

print("Sleeping after FOX timeline. Do not disturb!")
Sys.sleep(900)

## Get timelines of CNN followers. The limit is acctually 180 but better safe than sorry. It is so because it is app limit not timelines. In other words app can connect to Twitter only 180 times within 15 minutes

CNN_followers <- read_feather("CNN_followers")
CNN_timeline_all <- data_frame(screen_name="")
j <- 0
for (i in CNN_followers %>% filter(protected!=TRUE) %$% screen_name) {
  CNN_timeline <- get_timeline(i, n = 100)
  CNN_timeline_all <- bind_rows(CNN_timeline_all,CNN_timeline)
  ## if rate limit exhausted, then wait to rate limit reset
  if (j / 179 == j %/% 179) {
    print("Sleeping, during CNN timelines. Do not disturb!")
    Sys.sleep(900)
  }
  print(j <- j+1)
}

write_feather(CNN_timeline_all %>% select(-hashtags),"CNN_timelines_all")
write_as_csv(CNN_timeline_all,"CNN_timeline_all.csv")
print("Sleeping after CNN timeline. Do not disturb!")
Sys.sleep(900)
```

```{r group_users}
FOX_following_CNN_TRUE <- read_feather("Data/FOX_following_CNN_all") %>%
  filter(relationship=="target") %>%
  filter(value==TRUE) %$%
  user

FOX_following_CNN_FALSE <- read_feather("Data/FOX_following_CNN_all") %>%
  filter(relationship=="target") %>%
  filter(value==FALSE) %$%
  user

CNN_following_FOX_TRUE <- read_feather("Data/CNN_following_FOX_all") %>%
  filter(relationship=="target") %>%
  filter(value==TRUE) %$%
  user
CNN_following_FOX_FALSE <- read_feather("Data/CNN_following_FOX_all") %>%
  filter(relationship=="target") %>%
  filter(value==FALSE) %$%
  user

single_source <- read_twitter_csv("Data/FOX_timeline_all.csv") %>%
  filter(screen_name %in% FOX_following_CNN_FALSE) %>%
  mutate(following="FOX") %>%
  bind_rows(read_twitter_csv("Data/CNN_timeline_all.csv") %>% filter(screen_name %in% CNN_following_FOX_FALSE) %>% mutate(following="CNN")) %>%
  write_feather("Data/single_source")

double_source <- read_twitter_csv("Data/FOX_timeline_all.csv") %>%
  filter(screen_name %in% FOX_following_CNN_TRUE) %>%
  mutate(following="FOX") %>%
  bind_rows(read_twitter_csv("Data/CNN_timeline_all.csv") %>% filter(screen_name %in% CNN_following_FOX_TRUE) %>% mutate(following="CNN")) %>%
  unique() %>%
  write_feather("Data/double_source")
```

## Transform data set

Here we encounter a small problem. The emojis. I still did not figure out a good way of dealing with them. because we have a big data set here I would just get rid of them. The other concern is that we have followers who not necessarly are tweeting in English. For now I will get rid off them but a question maintans what to do with them. Transformation below are the same as we did in class, but might take a little while cause the data set is much bigger.

```{r transform_data}
single_source
```

## Complexity of sources

I would start with something fairly easy. It means with tf-idf

```{r complexity_sources}


```

## Emotional Complexity

## Complexity within one category

```{r category}
simple_authors <- NULL ## simple authors

complex_authors <- NULL ## complex authors
  
simple_authors <- get_timelines(complex_authors, n=100)

complex_authors <- get_timelines(complex_authors, n=100)
```

## Complexity of locations

```{r locations}
FOX_followers_timelines <- get_timelines(FOX_followers %>% slice(1:30) %$% user_id,n=10)

CNN_followers_timelines <- get_timelines(CNN_followers,n=100)

```

## Complexity of friends

```{r}

```

